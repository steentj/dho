# Titel: Lokal .env eksempel
# Oprettet: 2025-10-07 15:34
# Sidst ændret: 2025-10-07 15:34

# Lokal Udvikling Konfiguration
# Dette eksempel er perfekt til lokal udvikling med Ollama embeddings
# Ingen API omkostninger, god performance, offline kapabilitet

# =============================================================================
# DATABASE KONFIGURATION
# =============================================================================
# Lokal PostgreSQL database
POSTGRES_HOST=localhost
POSTGRES_USER=steen
POSTGRES_PASSWORD=********
POSTGRES_PORT=5432
POSTGRES_DB=WW2

# =============================================================================
# EMBEDDING PROVIDER KONFIGURATION
# =============================================================================
# Brug Ollama for lokale embeddings (ingen API omkostninger)
PROVIDER=ollama

# Alternativ til tests med OpenAI
# PROVIDER=openai

# Ollama konfiguration - lokalt kørende
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=nomic-embed-text

# OpenAI variabler (ikke brugt med Ollama, men kan være sat for testing)
OPENAI_API_KEY=sk-proj-test-key-for-future-use
OPENAI_MODEL=text-embedding-3-small

# =============================================================================
# TEKST PROCESSERING KONFIGURATION
# =============================================================================
# Mindre chunks for hurtigere lokal processering
CHUNK_SIZE=300

# Word overlap giver konsistente chunks ved lange dokumenter
CHUNKING_STRATEGY=word_overlap

# =============================================================================
# PROCESSERING KONFIGURATION
# =============================================================================
# Input fil til bog processering
URL_FILE=test_input.txt

# =============================================================================
# LOGGING KONFIGURATION
# =============================================================================
# Detaljeret logging for udvikling
LOG_LEVEL=DEBUG
LOG_DIR=./logs

# =============================================================================
# SØGEMASKINE API KONFIGURATION
# =============================================================================
# Tillad lokale development ports
TILLADTE_KALDERE=http://localhost:3000,http://127.0.0.1:3000,http://localhost:8080,http://localhost:8081

# Lavere threshold for bredere søgeresultater under udvikling
DISTANCE_THRESHOLD=0.5

# =============================================================================
# UDVIKLINGS SPECIFIKKE INDSTILLINGER
# =============================================================================
# Disse variabler er specifikke for lokal udvikling

# Development mode flag (hvis brugt i koden)
# DEVELOPMENT_MODE=true

# Reduced batch sizes for testing
# MAX_CONCURRENT_BOOKS=3

# =============================================================================
# DOCKER DEVELOPMENT NOTER
# =============================================================================
# Hvis du bruger Docker Compose til udvikling, ændr:
# POSTGRES_HOST=postgres
# OLLAMA_BASE_URL=http://ollama:11434
#
# For at starte development services:
# cd soegemaskine
# docker-compose --profile embeddings up -d postgres ollama
#
# For at installere Ollama model:
# ../scripts/setup_ollama.sh

# =============================================================================
# HURTIG START INSTRUKTIONER
# =============================================================================
# 1. Kopiér denne fil til roden: cp documentation/EKSEMPLER/.env.lokalt.eksempel .env
# 2. Start PostgreSQL lokalt eller via Docker
# 3. Start Ollama: docker run -d -p 11434:11434 ollama/ollama
# 4. Installér model: curl http://localhost:11434/api/pull -d '{"name":"nomic-embed-text"}'
# 5. Validér: ./scripts/process_books.sh --validate
# 6. Test: echo "https://example.com/test.pdf" > test_input.txt && ./scripts/process_books.sh --file test_input.txt
